---
title: "Logit_Ainsley"
author: "Ainsley McLaughlin"
date: "2024-08-06"
output: html_document
---

```{r}
everything_data<-read.csv("DrContactsNew.csv")
head(everything_data)
```

#
```{r}
#everything updated
everything_data2<-everything_data%>%
  mutate(condition = case_when(
    health %in% c("poor", "fair") ~ "unhealthy",
    health %in% c("good", "excellent") ~ "healthy",
    TRUE ~ as.character(health)
  ))

View(everything_data2)
```
#

```{r}
# make all binary zero or 1
everything_data3<-everything_data2%>%
  mutate(physlim=ifelse(physlim=="TRUE", 1,0),
         idp=ifelse(idp=="TRUE", 1,0),
         sex=ifelse(sex=="male", 1,0),
         child=ifelse(child=="TRUE", 1,0),
         black=ifelse(black=="TRUE", 1,0),
         condition=ifelse(condition=="healthy", 1,0))
View(everything_data3)
```
#
```{r}
#see columns of everything data:
colnames(everything_data3)  
```
# subset the data so it it using the log info and updated binary
```{r}
# took out x's and health which is condition and all exp and sqrted
log_data<-everything_data3[,-c(1,2,10,18:27)]
colnames(log_data)
```
# use aic with logit for response as "black"

```{r}
# build the logit model with everything but hthe response itself
logit_model1<-glm(physlim~.-physlim, log_data, family='binomial')
summary(logit_model1)
#looks like some are not helpful predictors
```
#
```{r}
# aic to choose predictors
aic<-MASS::stepAIC(logit_model1, direction='both', trace=F)
summary(aic)
# looks like it took out the insignificant ones: age (child also seems insinificant)
```
# look at multicollinearity and vif of variables that aic kept and without the response
```{r}
# also take out categorical
colnames(log_data)

dat<-log_data[,-c(6)]
cor_mat<-round(cor(dat),2)
ggcorrplot::ggcorrplot(cor_mat, lab=T, type='lower')
```
#

```{r}
cor(log_data[,"log_coinsurance"],log_data[,"log_max_deductible"])
# looks like significant multicollinearity between those two vairables, I will decide to throw one of those out too in my final logit model
```
# Also look at VIF and throw out the high ones

```{r}
# look at vif
logit_model2<-glm(physlim ~ visits + log_coinsurance + idp + log_api + 
    log_max_deductible + num_disease + log_fam_size + schooling + 
    child + black + condition, family = "binomial", data = log_data)
car::vif(logit_model2)
# confirms one of coinsurance or max deductable needs to go
```

#
# threw out coinsurance
```{r}
# final model (took out coinsurance)
logit_model_final<-glm(physlim ~ visits + idp + log_api + 
    log_max_deductible + num_disease + log_fam_size + schooling + 
    child + black + condition, family = "binomial", data = log_data)
summary(logit_model_final)
```
```{r}
# redo vif:
car::vif(logit_model_final)
# see all vif's very reasonable
```
```{r}
#colnames(log_data)
# make a prediction
# new data without the response variable--SHOULD I TAKE IT OUT???
# 1 is no, 27 is yes in reality, took out col 6= physlim
new_dat<-log_data[c(1,27), -6]
predict(logit_model_final,new_dat,type = "response")
```
# Interpretation:




# NOTE: STILL NEED TO CHECK ASSUMPTIONS (not necessarily that important) 
```{r}
long<-gather(log_data, key='predictor', value='value', visits, idp, log_api, log_max_deductible, 
    num_disease, log_fam_size, schooling, child, black,
    condition)
ggplot(long, aes(x=value, y=physlim, color=predictor))+geom_point()+
  facet_wrap(~predictor, scales='free_x')
```

# Also check accuracy with confusion matrix 
```{r}
add_data<-mutate(log_data, prob=predict(logit_model_final, type="response"), 
             log_odds=log(prob/(1-prob)),
             classify=ifelse(prob>0.5, 1,0))
table(add_data$classify, add_data$physlim)
```
# accuracy:16499+433/all= 16932/20186=0.8387

# Also build ROC curve - look at this for sure

```{r}
# tenfold cv
# Ensure the response variable is a factor with two levels
log_data$physlim_words<-everything_data3$physlim
log_data$physlim_words <- factor(log_data$physlim_words)
levels(log_data$physlim_words) <- make.names(levels(log_data$physlim_words))

ctrl <- trainControl(method = "cv", 
                     number = 10, 
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE, 
                     savePredictions = TRUE)

logit_model_final2 <- train(physlim_words ~ visits + idp + log_api + 
                            log_max_deductible + num_disease + log_fam_size + schooling +  child + black + condition, 
                            data = log_data, 
                            method = 'glm', 
                            family = 'binomial', 
                            trControl = ctrl, 
                            metric = "ROC")
# it is same as above but it includes the cross val and roc curve
summary(logit_model_final2)
```
#

```{r}
# create column called predictions
predictions<-logit_model_final2$pred
predictions
```
#
# get confusion matrix

```{r}
# but this one has been cross validated
table(predictions$pred, predictions$obs)
# picks anything above 50% as win and below is loss for the preditions
```
# Accuracy with cross validated model: 
16499+432/(all)=16931/20186=0.83875
# ROC
```{r}
library(pROC)
roc_thing<-roc(predictions$obs, predictions$X1)
roc_thing
```
#
```{r}
# tell it TP rate (sensitivity) and FP rate (1-specificity)
roc_df<-data.frame(TPR=roc_thing$sensitivities, FPR=1-roc_thing$specificities)
ggplot(roc_df, aes(x=FPR, y=TPR))+geom_line(color="blue")
```

#-----------------------------------------------------------------------
# TRY USING UNLOGGED DATA:
# took out x's and health which is condition and then only keep unlogged values
```{r}
unlog_data<-everything_data3[,-c(1,2,4,6,7,10, 11,12,23:27)]
colnames(unlog_data)
```

#
# use aic with logit for response as physlim

```{r}
# build the logit model with everything but hthe response itself
logit_model_unlog<-glm(physlim~.-physlim, unlog_data, family='binomial')
summary(logit_model_unlog)
#looks like some are not helpful predictors
```

#
```{r}
# aic to choose predictors
aic2<-MASS::stepAIC(logit_model_unlog, direction='both', trace=F)
summary(aic2)
# looks like it took out the insignificant ones: age (child also seems insinificant)
```

#
# look at multicollinearity and vif of variables that aic kept and without the response
```{r}
model_from_aic<-glm(formula = physlim ~ visits + idp + num_disease + schooling + 
    child + black + insurance + payment + deductible + income + 
    condition, family = "binomial", data = unlog_data)
summary(model_from_aic)
```
# Check multicollinearity
```{r}
car::vif(model_from_aic)
```
#
# NOTE: STILL NEED TO CHECK ASSUMPTIONS (not necessarily that important) 
```{r}
long<-gather(unlog_data, key='predictor', value='value', visits, idp, num_disease, schooling,
    child, black, insurance, payment, deductible, income, 
    condition)
ggplot(long, aes(x=value, y=physlim, color=predictor))+geom_point()+
  facet_wrap(~predictor, scales='free_x')
```

#

```{r}
# tenfold cv
# Ensure the response variable is a factor with two levels
unlog_data$physlim_words<-everything_data3$physlim
unlog_data$physlim_words <- factor(unlog_data$physlim_words)
levels(unlog_data$physlim_words) <- make.names(levels(unlog_data$physlim_words))

ctrl <- trainControl(method = "cv", 
                     number = 10, 
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE, 
                     savePredictions = TRUE)

logit_model_final2 <- train(physlim_words ~ visits + idp + num_disease + schooling + child + black + insurance + payment + deductible + income + 
    condition, 
                            data = unlog_data, 
                            method = 'glm', 
                            family = 'binomial', 
                            trControl = ctrl, 
                            metric = "ROC")
# it is same as above but it includes the cross val and roc curve
summary(logit_model_final2)
```

#
```{r}
# add a column that will give prob of win for each row
unlog_data2<-mutate(unlog_data, prob=predict(logit_model_final2$finalModel, type="response"), log_odds=log(prob/(1-prob)))

ggplot(unlog_data2, aes(x=visits, y=log_odds))+geom_point()# could do jitter
# have to make a similar plot for all the predictors can do a facet wrap
# see it is linearly related
# stuck with categorical predictors, it won't be linear
# Also build ROC curve - look at this for sure
```
#
```{r}
# Reshape the data frame to long format
unlog_data_long <- unlog_data2 %>%
  pivot_longer(cols = c(visits, idp, num_disease, schooling, child, black, insurance, payment, deductible, income, condition), 
               names_to = "predictor", 
               values_to = "value")

# Create the facet wrap plot
ggplot(unlog_data_long, aes(x = value, y = log_odds)) +
  geom_point() + 
  facet_wrap(~ predictor, scales = "free_x") +
  labs(x = "Predictor Value", y = "Log Odds") +
  theme_minimal()
```

# get confusion matrix

```{r}
# but this one has been cross validated
table(predictions$pred, predictions$obs)
# picks anything above 50% as win and below is loss for the preditions
```
# Accuracy:
```{r}
(16493+416)/(16493+416+254+3023)


```
# ROC
```{r}
library(pROC)
roc_thing<-roc(predictions$obs, predictions$X1)
roc_thing
```
#
```{r}
# tell it TP rate (sensitivity) and FP rate (1-specificity)
roc_df<-data.frame(TPR=roc_thing$sensitivities, FPR=1-roc_thing$specificities)
ggplot(roc_df, aes(x=FPR, y=TPR))+geom_line(color="blue")
```
# PREDICTIONS AND EXPONENENTS
```{r}
# we used this only with glm
exp(coef(logit_model_final2$finalModel))
```
# interpretation:

For schooling=0.9557047: For every one percentage point increase in schooling, the odds of a physical limitation decrease by (1-0.9557047=0.0442953)= 4.43% holding all other variables fixed. 

For number of diseases=1.0918321: for every one percentage point increase in number of diseases, the odds of a physical limitation increases by (1.0918321-1=0.0918321) 9.18% holding all other variables fixed. 

```{r}
# create column called predictions
predictions<-logit_model_final2$pred
predictions
```
I
# Interpreations:
For the values in row 1 there is a 89% chance the person has no physlim and a 10.5% chance they do. It also includes a predicted value and an observed value. 

```{r}
View(unlog_data)
```
#


#